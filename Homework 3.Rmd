---
title: "Homework 3, Lab 3, Econ B2000"
author: ' Mostafa Ragheb, Group Members: Isabela Vieira, Akimawe Kadiri, Nicole Kerrison,Christopher Tinevra, Charles Reed'
date: "10/05/2020"
output: github_document
---

Lab results will be gathered from using a simple k-nn technique to try in classifying people's neighborhoods, there will be multiple runs utlizing different categories.

For the first example, this run will follow the variables of household cost anf income total which were provided from the Lab 3 guidelines:

```{r}
load("/Users/mostafaragheb/Desktop/CCNY/4th ( Fall 2020)/Economatrics/HW1/acs2017_ny/acs2017_ny_data.RData")
dat_NYC <- subset(acs2017_ny, (acs2017_ny$in_NYC == 1)&(acs2017_ny$AGE > 18) & (acs2017_ny$AGE < 66))
attach(dat_NYC)
borough_f <- factor((in_Bronx + 2*in_Manhattan + 3*in_StatenI + 4*in_Brooklyn + 5*in_Queens), levels=c(1,2,3,4,5),labels= c("Bronx","Manhattan","Staten Island","Brooklyn","Queens"))
```

```{r}
norm_varb <- function(X_in) {
  (max(X_in, na.rm = TRUE) - X_in)/( max(X_in, na.rm = TRUE) - min(X_in, na.rm = TRUE) )
}
is.na(OWNCOST) <- which(OWNCOST == 9999999)
housing_cost <- OWNCOST + RENT
norm_inc_tot <- norm_varb(INCTOT)
norm_housing_cost <- norm_varb(housing_cost)
data_use_prelim <- data.frame(norm_inc_tot,norm_housing_cost)
good_obs_data_use <- complete.cases(data_use_prelim,borough_f)
dat_use <- subset(data_use_prelim,good_obs_data_use)
y_use <- subset(borough_f,good_obs_data_use)
set.seed(12345)
NN_obs <- sum(good_obs_data_use == 1)
select1 <- (runif(NN_obs) < 0.8)
train_data <- subset(dat_use,select1)
test_data <- subset(dat_use,(!select1))
cl_data <- y_use[select1]
true_data <- y_use[!select1]
summary(cl_data)
prop.table(summary(cl_data))
summary(train_data)
summary(test_data)
suppressMessages(require(class))
for (indx in seq(1, 9, by= 2)) {
 pred_borough <- knn(train_data, test_data, cl_data, k = indx, l = 0, prob = FALSE, use.all = TRUE)
num_correct_labels <- sum(pred_borough == true_data)
correct_rate <- (num_correct_labels/length(true_data))*100
print(c(indx,correct_rate))
print(summary(pred_borough))
}
```

For the second run, we will be using the categories of Poverty, Family size and Household income for possible classification.

```{r}
fam_pov <- POVERTY + FAMSIZE
norm_houseinc <- norm_varb(HHINCOME)
norm_fam_pov <- norm_varb(fam_pov)
data_use_prelim <- data.frame(norm_houseinc,norm_fam_pov)
good_obs_data_use <- complete.cases(data_use_prelim,borough_f)
dat_use <- subset(data_use_prelim,good_obs_data_use)
y_use <- subset(borough_f,good_obs_data_use)
set.seed(12345)
NN_obs <- sum(good_obs_data_use == 1)
select1 <- (runif(NN_obs) < 0.8)
train_data <- subset(dat_use,select1)
test_data <- subset(dat_use,(!select1))
cl_data <- y_use[select1]
true_data <- y_use[!select1]
summary(cl_data)
prop.table(summary(cl_data))
summary(train_data)
summary(test_data)
suppressMessages(require(class))
for (indx in seq(1, 9, by= 2)) {
 pred_borough1 <- knn(train_data, test_data, cl_data, k = indx, l = 0, prob = FALSE, use.all = TRUE)
num_correct_labels <- sum(pred_borough1 == true_data)
correct_rate <- (num_correct_labels/length(true_data))*100
print(c(indx,correct_rate))
print(summary(pred_borough1))
}
```
In the second run, the k-nn algorithm gave an output accuracy higher than the first run sequences. Also, the distribution of the people had a similar count per borough in comparison to the first run. 

For the third run, we will be using the categories of cost utilities including water, heat, gas & electricity and Household income for possible classification.
```{r}
utility_cost <- COSTELEC + COSTFUEL + COSTGAS + COSTWATR
norm_houseinc2 <- norm_varb(HHINCOME)
norm_utility_cost <- norm_varb(utility_cost)
data_use_prelim <- data.frame(norm_houseinc2,norm_utility_cost)
good_obs_data_use <- complete.cases(data_use_prelim,borough_f)
dat_use <- subset(data_use_prelim,good_obs_data_use)
y_use <- subset(borough_f,good_obs_data_use)
set.seed(12345)
NN_obs <- sum(good_obs_data_use == 1)
select1 <- (runif(NN_obs) < 0.8)
train_data <- subset(dat_use,select1)
test_data <- subset(dat_use,(!select1))
cl_data <- y_use[select1]
true_data <- y_use[!select1]
summary(cl_data)
prop.table(summary(cl_data))
summary(train_data)
summary(test_data)
suppressMessages(require(class))
for (indx in seq(1, 9, by= 2)) {
 pred_borough2 <- knn(train_data, test_data, cl_data, k = indx, l = 0, prob = FALSE, use.all = TRUE)
num_correct_labels <- sum(pred_borough2 == true_data)
correct_rate <- (num_correct_labels/length(true_data))*100
print(c(indx,correct_rate))
print(summary(pred_borough2))
}
```
In the third run, the k-nn algorithm gave an output accuracy much higher in comparison to the second run sequences and first run sequences. Additionally, the distribution of the people had a similar count per borough in comparison to the first run. 

In conclusion, we are using the k-nearest neighbor algorithm to predict the living location of a random individual in our dataset. We first start by restricting our dataset to people living in NY (since pums data reflects the entire state of NY), so we can classify 80% of our data as living in one of the 5 boroughs of NYC. The likeability that a person lives in Brooklyn is 35%, Bronx is 14%, Manhattan is 15%, Queens is 31%. Using that probability distribution and other 2 or more variables of our choice, we try to predict the living location of the remaining 20% of our data.

That prediction is based on the proximity of a random non-classified datapoint (individual) to other classified data points. For example, if the majority of the houses in Manhattan have a very high utility cost, then if a random datapoint has a high utility we assume that household to be located in Manhattan. The level of accuracy we achieve from the second run using variables of Poverty, family size and household income gave us a 61.85% for the first sequence. While the level of accuracy we achieve from the third run using variables of cost of multiple utilities  household income gave us a 79.87% for the first sequence. Therefore, utilizing more information or variables with the k-nn  algorithm from the categories within the dataset can lead to higher accuracy to predict the living location of random individuals. 

